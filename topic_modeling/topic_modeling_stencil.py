# -*- coding: utf-8 -*-
"""Topic Modeling Stencil.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ubXbNHxlm8cXNnuFnhomNImPW46odl9w

# Introduction

In this project we will leverage skills such as Topic Modeling, Latent Dirichlet Allocation to build a small model analyzing the content of news articles from different sources around the US.

The main objectives for this project are:
1. Use LDA topic modeling to find patterns in a realistic, noisy, unlabeled text corpus
2. Understand how topic modeling results are influenced by preprocessing and hyperparemters
3. Use dimensionality reduction and clustering algorithms to create effective visualizations of large text data

# Installation and Imports
"""

# Mount the drive to be able to read and write files from your drive
from google.colab import drive
drive.mount('/content/drive')

# Python Import Statements
import re
from typing import *
import collections
import sys
import math
import numpy as np
import csv
import spacy
from tqdm import tqdm
from os.path import exists
import pickle
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt

"""# Part 1: Basic Topic Model

First, let's train a basic topic model and see what happens! We'll do very basic text preprocessing (tokenization and lower casing) to start, and we'll use LSA for the topic model.

You can download the dataset [here](https://drive.google.com/file/d/1146a1rhJ95NEySuMS000KDRPcat-itPh/view?usp=sharing) - we've handled loading it below.

**Warning**: Loading the data could take as much as 10 minutes when running for the first time. After that, the parsed documents are cached in a pickle file...
"""

######################
#     LOAD DATA      #
#############################################################################################################################
# This cell block will load in the dataset into                                                                             #
#   * spacy_processed_docs - A list of Spacy Documents that we can use as data to train our Topic Model                     #
#   * doc_locations - A list of States where each Document was sourced from (This will be used in Part 3: Visualization)    #
#############################################################################################################################

num_documents = 5000
FILEPATH = "/content/drive/MyDrive/topic_modeling/"

nlp = spacy.load('en_core_web_sm')

doc_locations = []
spacy_processed_docs = []

if exists(f"{FILEPATH}spacy_processed_docs_{num_documents}.pkl"):
  with open(f"{FILEPATH}spacy_processed_docs_{num_documents}.pkl", 'rb') as f:
    spacy_processed_docs, doc_locations = pickle.load(f)
    f.close()
else:
  with open(f'{FILEPATH}articles_sampled_data.csv', 'r', encoding='utf-8') as f:
    for i, row in tqdm(enumerate(csv.DictReader(f, delimiter=','))):
      if i == num_documents:
        break
      if i % 500 == 0:
        print("Processing row %d"%i)
      try:
        parsed = nlp(row["content"])
        source_name = row["location"]
      except ValueError:
        continue
      spacy_processed_docs.append(parsed)
      doc_locations.append(source_name)
    f.close()

  with open(f"{FILEPATH}spacy_processed_docs_{num_documents}.pkl", 'wb') as f:
    pickle.dump((spacy_processed_docs, doc_locations), f)
    f.close()

print("PROCESSED")
print(spacy_processed_docs[0])
print(f"\nNumber of documents: {len(spacy_processed_docs)}")

"""To start, let's build the following components for topic modeling:

1.  **M**:
    A binary term-document matrix of shape (num_documents, vocab_size)

2.  **word2idx**: A dictionary which maps each word to its rank in the vocabulary (e.g. the most frequent word should have rank 0, the second most frequent word rank 1, etc).

3.  **idx2word**: The inverse of the above (i.e., mapping from index to word)

We will use **M** to train the topic model directly, and we will use the other two lookup tables in order to analyze the actual topics produced.
"""

def binary_term_doc_matrix(docs : List[spacy.tokens.Doc]) -> Tuple[np.ndarray[np.float64], Dict[int, str]]:
  """
  Preprocess and transform docs to create our binary term-document matrix and dictionaries as described above

  Parameters
  ----------
  docs : List[Doc]
    A list of spacy processed documents (i.e. each item is the output of nlp(article))

  Returns
  -------
  M : np.ndarray[float]
    The binary term-document matrix, each value in M should be either 0 or 1
  idx2word : Dict[int, str]
    The dictionary that maps each index/rank to each word in the vocabulary
  """
  word2idx = {}
  idx2word = {}
  counter = Counter(token.lemma_.lower() for doc in docs for token in doc)
  vocab_size = len(counter.keys())
  sorted_keys = [item[0] for item in counter.most_common()]
  for i, word in enumerate(sorted_keys):
    word2idx[word] = i
    idx2word[i] = word
  M = np.zeros((len(docs), vocab_size))
  for i, doc in enumerate(docs):
    for token in doc:
      if token.lemma_.lower() in word2idx:
        M[i, word2idx[token.lemma_.lower()]] = 1
  return M, idx2word

M,idx2word = binary_term_doc_matrix(spacy_processed_docs)
print(M.shape)

from sklearn.decomposition import LatentDirichletAllocation

def train_topic_model(term_doc_mat : np.ndarray[np.float64], n_topics : int = 10, random_state = 42) -> LatentDirichletAllocation:
  """
  Train a n_topics topic model on M using Latent Dirichlet Allocation
  Use LDA to fit a model with n_topics, then return the model

  Parameters
  ----------
  term_doc_mat : np.ndarray[float]
    The term-document matrix to train the LDA model on
  n_topics : int
    The number of topics in the topic model (Defaulted to 10)
  random_state : int
    The random state of the LDA Model (Defaulted to 42)

  Returns
  -------
  lda : LatentDirichletAllocation
    The trained LDA model
  """
  lda = LatentDirichletAllocation(n_components=n_topics,
    random_state=random_state)
  lda.fit(term_doc_mat)
  return lda

def preview_topics(topic_model: LatentDirichletAllocation, idx2word: Dict[int, str]) -> List[List[str]]:
  """
  Print out/return the top 10 words associated with each topic

  Parameters
  ----------
  topic_model : LatentDirichletAllocation
    The trained LDA Topic Model
  idx2word : Dict[int, str]
    The dictionary that maps each index/rank to each word in the vocabulary

  Returns
  -------
  topics : List[List[str]]
    A list of the 10 words associated with each topic
  """
  topics = []
  for topic in topic_model.components_:
    topic_words = []
    sorted_index = np.argsort(topic)[::-1]
    for i in sorted_index[:10]:
      topic_words.append(idx2word[i])
    topics.append(topic_words)
  return topics

###############################################
topic_model = train_topic_model(M, n_topics=10)
preview_topics(topic_model, idx2word)

#1m29s to run
###############################################

"""# Part 2: Improved Topic Model

Now let's try to improve the quality of the topics by improving the features our model can use (as opposed to binary features).

1. First, we'll write a helper function to preprocess a spacy document. Here, we only want use ***LOWERCASE, REMOVE NEWLINES*** `(token.pos_=='SPACE')`, ***REMOVE PUNCTUATION***, and ***REMOVE STOPWORDS***.


2.  Then, we will implement TFIDF.


TFIDF is the product of two statistics:

1. **Term Frequency (tf)**: The relative frequency of a term $w$ in a document $d$. We will use the following formula:
  
$$
    tf = 0.5 + \left(0.5 \cdot \frac{f_w}{\max\{f_w' : w' \in d\}}\right)
$$
  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;where $f_w$ is the frequency $w$ in document $d$.
  
  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;For each document, each word in that document has a tf value. For words not in that document ***the tf value should be 0 NOT 0.5***

  

2. **Inverse Document Frequency (idf)**:  For the whole corpus of documents $D$, how many of the documents does the term $w$ appear?        

  Intuitively, this is how much information a word provides if it appears in a document.
  We will use the following formula:

$$
    idf = log(\frac{|D|}{|d\ \in D: w\ \in d|})
$$

  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;where the denominator indicates the number of documents that a term $w$ appears in.
  
  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Note that there is one idf value for each word in the vocab.
  
  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Hint: You will have to iterate over all of the documents to calculate idf prior to remaking your matrix M

Finally, the TFIDF score is calculated by multiplying these two values together:
$$
    TFIDF = tf * idf
$$

With this in mind, Let's reconstruct **M**, **word2idx**, and **idx2word**.
This time, let's make the following changes:
1. Filter out ***STOPWORDS***, ***PUNCTUATION***, ***NEWLINES*** and make everything ***LOWERCASE*** using spacy
2. Filter the vocabulary to only the ***MOST FREQUENT*** 5000 words
3. Use TFIDF values instead of binary count to remake **M**
"""

from collections import Counter # HINT: you may find this useful
import math

def preprocess_doc(doc : spacy.tokens.Doc) -> List[str]:
  """
  Write a helper function that filters out STOPWORDS, PUNCTUATION, and NEWLINES from the spacy processed doc.
        Also LOWERCASE the tokens as well.

  Parameters
  ----------
  doc : Doc
    The spacy preprocessed document

  Returns
  -------
  preprocessed : List[Str]
    A list of the preprocessed strings
  """
  proc_doc = []
  for token in doc:
    if token.is_stop or token.is_punct or token.pos_ == 'SPACE':
      continue
    proc_doc.append(token.lemma_.lower())
  return proc_doc

def create_vocab(proc_docs : List[List[str]], vocab_cutoff : int) -> List[str]:
  """
  Aggregates and collects the text of the most common tokens in docs, cutoff by the vocab_cutoff.

  Parameters
  ----------
  proc_docs : List[List[str]]
    A list of preprocessed documents using preprocess_doc
  vocab_cutoff : int
    The cutoff of the MOST FREQUENT vocabulary

  Returns
  -------
  vocab : List[Str]
    A list of the top {vocab_cutoff} most common token texts in docs
  """
  vocab = []
  counter = Counter(token for doc in proc_docs for token in doc)
  for word, count in counter.most_common(vocab_cutoff):
    vocab.append(word)
  return vocab

def idf_matrix(proc_docs : List[List[str]], word2idx : Dict[str, int],  vocab : List[str]) -> np.ndarray[np.float64]:
  """
  Calculate the Inverse Document Frequency (IDF) Matrix using the equation above for each word
  Equation: idf(w) = log(|D| / |d in D : w in d|)

  Parameters
  ----------
  proc_docs : List[List[str]]
    A list of preprocessed documents using preprocess_doc
  word2idx : Dict[Str, Int]
    A dictionary that matches each word in the vocabulary to it's rank
  vocab : List[Str]
    The actual vocab of all the docs (thresholded by vocab_cutoff)

  Returns
  -------
  idf : np.array[Float]
    The IDF array as defined by the equation in the description
  """
  idf = np.zeros(len(vocab))
  num_docs = len(proc_docs)
  for word in vocab:
    doc_count = 0
    for doc in proc_docs:
      if word in doc:
        doc_count += 1
    idf[word2idx[word]] = math.log(num_docs / doc_count)
  return idf

def tf_matrix(proc_docs : List[List[str]], word2idx : Dict[str, int], vocab : List[str]) -> np.ndarray[np.float64]:
  """
  Calculate the Term Frequency (TF) Matrix using the equation above for each word
  Equation: tf(w, d) = 0.5 + 0.5 * (freq_w_in_d / freq_wmax_in_d)
  NOTE: For words not in the document the TF value should be 0 and NOT 0.5

  Parameters
  ----------
  proc_docs : List[List[str]]
    A list of preprocessed documents using preprocess_doc
  word2idx : Dict[Str, Int]
    A dictionary that matches each word in the vocabulary to it's rank
  vocab : List[Str]
    The actual vocab of all the docs (thresholded by vocab_cutoff)

  Returns
  -------
  tf : np.array[Float]
    The TF array as defined by the equation in the description
  """
  tf = np.zeros((len(proc_docs), len(vocab)))
  for i, doc in enumerate(proc_docs):
    for word in doc:
      if word in vocab:
        tf[i, word2idx[word]] += 1
  tf = 0.5 + 0.5 * tf / tf.max(axis=1)[:, np.newaxis]
  tf[tf == 0.5] = 0
  return tf

def tfidf_term_doc_matrix(docs : List[spacy.tokens.Doc], vocab_cutoff : int = 5000) -> Tuple[np.ndarray[np.float64], Dict[int, str]]:
  """
  There are multiple steps in this function:
  TODO:
    1. Create the vocab
    2. Threshold it by vocab_cutoff and compute IDF
    3. Compute TF for each word in the document
    4. Use TF and IDF to calculate TFIDF for each entry in M

  Parameters
  ----------
  docs : List[Doc]
    A list of spacy preprocessed documents
  vocab_cutoff : int
    The cutoff of the MOST FREQUENT vocabulary

  Returns
  -------
  M : np.ndarray[float]
    The TFIDF term document Matrix
  idx2word : Dict[int, str]
    The dictionary that maps each index/rank to each word in the vocabulary
  """

  word2idx = {}
  idx2word = {}
  proc_docs = [] # A list of preprocessed docs

  # Preprocess each document and compute your thresholded vocab
  for doc in docs:
    proc_docs.append(preprocess_doc(doc))
  vocab = create_vocab(proc_docs, vocab_cutoff)

  # Fill in word2idx and idx2word
  for i, word in enumerate(vocab):
    word2idx[word] = i
    idx2word[i] = word

  M = np.zeros((len(docs), len(vocab)))

  # Calculate the IDF array
  idf = idf_matrix(proc_docs, word2idx, vocab)

  # Calculate the TF array
  tf = tf_matrix(proc_docs, word2idx, vocab)

  # Combine the TF and IDF array to make the TFIDF array (M)
  M = tf * idf
  return M, idx2word

###############################################################
M, idx2word = tfidf_term_doc_matrix(spacy_processed_docs)

topic_model = train_topic_model(M, n_topics=10)
preview_topics(topic_model, idx2word)
###############################################################

"""Now let's tune the number of topics in order to determine the "right" number of topics.

To do this, we'll choose the number of topics that minimizes perplexity on held-out data. Specifically, we will need to do the following:
1. Split our data into 80% train and 20% dev
2. Using the training data, train 5 topic models, one for each of the following numbers of topics: [1, 5, 10, 15, 20]
3. For each trained model, compute the perplexity on the dev set, and plot the result
"""

def test_models(M : np.ndarray[np.float64], ks : List[int], dev_split : int = 0.2):
  """
  Tests out our models on multiple different values of k, where k is the number of topics for our model
  Then it returns a perplexity list for each value of k

  Parameters
  ----------
  M : np.ndarray[float]
    The input term document Matrix
  ks : List[int]
    A list of different k-values (numbers of topics) to test on
  dev_split : int
    How much you want to split your data for testing. In our case dev_split should be 0.2 so that we split
    our data into 80% train and 20% dev.

  Returns
  -------
  train_scores : List[float]
    The perplexity scores of each model in training
  dev_scores : List[float]
    The perplexity scores of each model in dev
  """
  train_data, dev_data = train_test_split(M, test_size=dev_split)

  train_scores = []
  dev_scores = []
  for k in ks:
    print("Training LDA model with %d topics..."%k)
    model = train_topic_model(M, n_topics=k)
    train_scores.append(model.perplexity(train_data))
    dev_scores.append(model.perplexity(dev_data))

  return train_scores, dev_scores

# Plot the perplexity of each topic model
ks = [1, 5, 10, 15, 20]
train_scores, dev_scores = test_models(M, ks)
plt.plot(np.arange(len(train_scores)), train_scores, label="Train")
plt.plot(np.arange(len(dev_scores)), dev_scores, label="Dev")
plt.xticks(np.arange(len(ks)), ['%d'%k for k in ks])
plt.legend()
plt.show()

"""# Part 3: Visualization

Now let's use the topic model we just trained to inspect the data visually.

First, Let's train the model with the amount of topics that minimized perplexity above...
"""

# Train the model on the full data using the number of topics we chose above (i.e., the value of k that minimized perplexity on dev).

best_k = 5 # The number of Topics that got the best perplexity
topic_model = train_topic_model(M, n_topics=best_k) # Train the topic model with best_k topics
topics = preview_topics(topic_model, idx2word)

"""To start, we want to visualize our Term Document Matrix, but that can be difficult to visualize because each document is represented by a 5000 component vector. Thus in order to visualize our Matrix, we will want to use **dimensionality reduction** to reduce the number of components to 2 so that we can plot each Document on a 2D grid.  

In the cell below, we'll make use of PCA (Principle Component Analysis), a process that uses SVD (Singular Value Decomposition) to extract out the 2 most important Principal Components of our data which we can use to visualize our Matrix.

- The output of `PCA.fit_transform()` is a matrix of shape `(n_samples, n_components)`, where `n_samples = #documents`, and `n_components = 2`, in our case. We will treat each row `i` of the output as 2D coordinates for document `i`.
- Check out the PCA documentation [here](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html).
"""

import plotly.express as px
import pandas as pd
from sklearn.decomposition import PCA

# NOTE: This code initialized our PCA model to project our matrix M into 2 dimensions
def pca_projection(M : np.ndarray[np.float64], n_components : int = 2) -> np.ndarray[np.float64]:
  """
  Use PCA to project M into n_components.

  Parameters
  ----------
  M : np.ndarray[float]
    The input Term Document Matrix
  n_components : int
    The number of components/dimensions that you want to reduce to

  Returns
  -------
  proj : np.ndarray[float]
    The projected matrix (M projected to n_components)
  """
  pca = PCA(n_components) # PCA model with n_components = 2
  proj = pca.fit_transform(M) # Projections: An np.array of shape (n_samples, n_components)
  return proj

"""With this projection we can now plot all of the documents on a 2d plane, but ideally we want each point to be colored by their "most prominent Topic" and contain some more identifying data about the document.

TODO: In order to achieve this, for each point lets create a function to build a dataframe **df** such that
1. Each row corresponds to one of the documents
2. The **topic** column maps each Document to it's "Most Prominent Topic"
3. The **text** column maps each Document to the a quick text snippet from the Document (the first 100 characters of the Document's raw text)
4. The **x** and **y** columns map to the (x, y) coordinates of the 2d projection

"""

def generate_datapoints(docs : List[spacy.tokens.Doc], proj: np.ndarray[np.float64], M : np.ndarray[np.float64], topic_model: LatentDirichletAllocation) -> pd.DataFrame:
  """
  Generates a Pandas Dataframe where each row corresponds to the data point (document). Specifically:
    1. The (x, y) coordinates of the projected document
    2. The text of the document (capped at 100 characters)
    3. The most prominent topic of the document (As a string of the topic number e.g. "2")

  Parameters
  ----------
  docs : List[Doc]
    A list of spacy preprocessed documents
  M : np.ndarray[float]
    The input Term Document Matrix
  proj: np.ndarray[float]
    The projection matrix which is M projected to 2d space using our previously defined pca_projection function
  topic_model: LatentDirichletAllocation
    The trained topic model

  Returns
  -------
  df : DataFrame
    The filled out Dataframe
  """
  df = pd.DataFrame(columns = ['topic', 'text', 'x', 'y'])

  topic_scores = topic_model.transform(M)

  # Fill out the dataframe with all of the columns filled out for each document (row).
  data = []
  for i, doc in enumerate(docs):
    data.append({
        'topic': str(np.argmax(topic_scores[i])),
        'text': doc.text[:100],
        'x': proj[i, 0],
        'y': proj[i, 1]
    })
  df = pd.DataFrame(data)
  return df

"""Now, run the following cell to plot the PCA projection of our documents onto 2D! Each point corresponds to a document, which we can hover over and view some of the contents of!"""

# This plots the plot_data list built in the previous cell
df = generate_datapoints(spacy_processed_docs, pca_projection(M, 2), M, topic_model)
fig = px.scatter(df, x="x", y="y", color="topic", hover_data=['text'])
fig.show()

"""## Geographical Visualization

Now, we'd like to visualize the most popular topics based on geographical location!

We'll start with a simple bar graph showing the popularity of topics by state, where the height of each bar is the proportion of documents from each location that correspond to a particular topic.

To visualize the most popular counts by location and topic we first need to group and count the number of documents that have that "most prominent topic" and who originate from that location.
"""

def generate_location_data(docs : List[spacy.tokens.Doc], M : np.ndarray[np.float64], locations : List[str], topic_model: LatentDirichletAllocation) -> Dict[int, Dict[str, int]]:
  """
  Aggregates the Documents that all share the same "Most Prominent Topic" and retains a count of the number of Documents at each location
  for each Document. This will be used to generate a Bar Chart visualization and Map Visualization of where each Topic is most popular.

  Parameters
  ----------
  docs : List[Doc]
    A list of spacy preprocessed documents
  M : np.ndarray[float]
    The input Term Document Matrix
  locations : List[str]
    A list that records the location of each document. Thus, the location of i-th document is doc_locations[i]
  topic_model: LatentDirichletAllocation
    The trained topic model

  Returns
  -------
  location_count : Dict[int, Dict[str, int]]
    A dictionary that maps each topic (int) to a Dictionary mapping Location (str) to Counts (int) for that topic.
    This is designed so that
      * location_count[topic][state] is the count of documents for that state and topic
  """
  location_count = {}

  topic_scores = topic_model.transform(M)

  topic_locations = {}
  for i, doc in enumerate(docs):
    topic = np.argmax(topic_scores[i])
    if topic not in topic_locations:
      topic_locations[topic] = []
    topic_locations[topic].append(locations[i])

  for key in topic_locations.keys():
    count = {}
    for location in topic_locations[key]:
      if location not in count:
        count[location] = 0
      count[location] += 1
    location_count[key] = count

  return location_count

"""With this, we can use the `generate_location_data` function to aggregate the number of locations for each topic and then generate our Bar Graphs. Nothing more needs to be done here."""

from collections import Counter

state_count = generate_location_data(spacy_processed_docs, M, doc_locations, topic_model)

# This code makes a bar graph displaying the popularity (as a proportion) per state...
for k in range(best_k):
  fig = plt.figure(figsize=(20,4))

  threshold = 30

  total_doc_counts = dict(Counter(doc_locations).items())
  normalized_state_count = {loc : 100 * state_count[k][loc] / total_doc_counts[loc] for loc in state_count[k] if total_doc_counts[loc] > threshold}
  state_items = sorted(normalized_state_count.items(), key = lambda item: item[1], reverse = True)
  state_names = [item[0] for item in state_items]
  count = np.array([item[1] for item in state_items])

  plt.bar(state_names, count, color ='blue', width = .8)
  plt.ylim([0, 100])
  plt.xticks(rotation=90)
  plt.xlabel("States")
  plt.ylabel("Percentage of headlines from this location")
  plt.title(f"Topic {k}: {', '.join(topics[k])}")
  plt.show()

"""Now, we'll use a library to plot the number of documents of each topic on a map, because it's much more visually intuitive than a bar graph!"""

from geopy.geocoders import Nominatim
import plotly.graph_objects as go

# Our geolocation service
geolocator = Nominatim(user_agent='myapplication')
for k in range(best_k):
  lon, lat = [], []

  threshold = 30

  total_doc_counts = dict(Counter(doc_locations).items())
  normalized_state_count = {loc : 100 * state_count[k][loc] / total_doc_counts[loc] for loc in state_count[k] if total_doc_counts[loc] > threshold}
  state_items = sorted(normalized_state_count.items(), key = lambda item: item[1], reverse = True)
  state_names = [item[0] for item in state_items]
  count = np.array([item[1] for item in state_items])

  labels = []
  # Assign each state a geographical location
  for state, c in zip(state_names, count):
    location = geolocator.geocode(state)
    lat.append(location.latitude)
    lon.append(location.longitude)
    labels.append(f"{state} : {c}% of all {state} headlines have this topic")

  marker = dict(color="blue", size=count)
  fig = go.Figure(data=go.Scattergeo(lon=lon, lat=lat, text=labels, mode='markers', marker=marker))
  fig.update_layout(title = f"Topic {k}: {', '.join(topics[k])}" ,geo_scope='usa')
  fig.show()

from collections import Counter

state_count = generate_location_data(spacy_processed_docs, M, doc_locations, topic_model)
for k in range(best_k):
  fig = plt.figure(figsize=(20,4))

  threshold = 50
  total_doc_counts = dict(Counter(doc_locations).items())
  normalized_state_count = {loc : 100 * state_count[k][loc] / total_doc_counts[loc] for loc in state_count[k] if total_doc_counts[loc] > threshold}
  state_items = sorted(normalized_state_count.items(), key = lambda item: item[1], reverse = True)
  state_names = [item[0] for item in state_items]
  count = np.array([item[1] for item in state_items])

  plt.bar(state_names, count, color ='blue', width = .8)
  plt.ylim([0, 100])
  plt.xticks(rotation=90)
  plt.xlabel("States")
  plt.ylabel("Percentage of headlines from this location")
  plt.title(f"Topic {k}: {', '.join(topics[k])}")
  plt.show()