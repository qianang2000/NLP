# -*- coding: utf-8 -*-
"""BOW Stencil.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1s4nKu9phjXXSjnZ1-bUo2Pbx70HafXP_

# Introduction

In this report, we will be implementing the very first NLP model (Woohoo!) -- a Bag of Words (BOW) model for sentiment classification. We will be using a very useful NLP libary (spaCy), and a popular machine learning library (scikit-learn, or sklearn) to preprocess the data and build the BOW model.

The main steps for this report are:

1. Use sklearn to implement a standard ML workflow (featurization, designing train-test splits, training a model, and evaluating the model).
2. Use spaCy for standard NLP preprocessing steps (tokenization, lemmatization, tagging).
3. Understand and implement common featurization approaches for text classifiers (ngrams, weighting schemas, tagging).

The basic workflow for an NLP model contains the following steps:

1. Load and Explore Data
2. Preprocess Data
3. Extract Features
4. Train the Model
5. Evaluate the Model
6. Analyze Model Behavior

### Installation and Imports
"""

!pip install spacy
!pip install sklearn

import os
import spacy

from google.colab import drive

import numpy as np
import pandas as pd
import pickle
from typing import List, Tuple, TypeAlias

drive.mount("/content/drive", force_remount=True)

"""# Part 1: BOW Workflow

### Step 1: Load the raw data from disk and explore the data

In this assignment, we will use a Twitter dataset with social media text. In the dataset, the tweets are rated for three categories of sentiment: positive, negative and neutral. We will use the tweets in the `text` column and the sentiments in the `sentiment` column.
"""

FOLDER = "/content/drive/My Drive"  # Extend this string to point to the directory with your copy of Tweets_5K.csv
FILEPATH = f"{FOLDER}/Tweets_5K.csv"
pd.read_csv(
    FILEPATH,
    usecols=[1, 2],
)

def load_data() -> Tuple[List[str], List[int]]:
    """
    Loads Twitter data into two lists.

    Returns
    -------
    raw_tweets : List[str]
        A list of all Tweets in the dataset
    labels : List[int]
        A list of the sentiments corresponding to each raw tweet encoded as integers,
        -1 meaning negative, 0 meaning neutral, and 1 meaning positive
    """
    data = pd.read_csv(FILEPATH, usecols=[1, 2],)
    raw_tweets = data['text'].tolist()
    map = {"negative": -1, "neutral": 0, "positive": 1}
    labels = data["sentiment"].replace(map).tolist()
    return raw_tweets, labels

raw_tweets, labels = load_data()
for p, label in zip(raw_tweets[:10], labels[:10]):
    print(f"{label}:\t{p}\n")

"""We can use matplotlib to see the ratio between positive, negative and neutral sentiments.

"""

import matplotlib.pyplot as plt

plt.figure()
pd.value_counts(labels).plot.bar(title="Sentiment Distribution in Tweets")
plt.xlabel("Sentiment")
plt.ylabel("Number of Tweets")
plt.show()

"""### Step 2: Preprocess the data

For now, all we need to do for preprocessing is split the tweets by whitespace.
"""

def preprocess(raw_X: List[str]) -> List[List[str]]:
    """
    Performs splitting on whitespace on all raw strings in a list.

    Parameters
    ----------
    raw_X : List[str]
        A list of raw strings (tweets)

    Returns
    -------
    List[List[str]]
        A list of preprocessed tweets (which are now lists of words)
    """
    words = [x.split(" ") for x in raw_X]
    return words

"""### Step 3: Define the Model

"""

from scipy.sparse import csr_matrix
from sklearn.feature_extraction import DictVectorizer
from sklearn.linear_model import LogisticRegression


class BOW_Classifier:
    """
    Attributes
    ----------
    clf : LogisticRegression
        A logistic regression classifier
    dv : DictVectorizer
        A dictionary vectorizer for turning dictionaries into matrices
    """

    def __init__(self):
        self.clf = LogisticRegression(penalty='l2', tol=0.0001, C=1.0, max_iter=150)
        self.dv = DictVectorizer(sparse=True)

    def featurize(
        self, preproc_X: np.ndarray[List[str]], is_test: bool = False
    ) -> csr_matrix:
        """
        Turns a list of preprocessed tweets into a binary bag of words
        matrix.

        Parameters
        ----------
        preproc_X : np.ndarray[List[str]]
            A list of preprocessed tweets
        is_test: bool, default=False
            Whether featurization should be done using features learned during training (is_test=True)
            or whether it should be done with features extracted from scratch using preproc_X (is_test=False)

        Returns
        -------
        csr_matrix
            A matrix with rows corresponding to tweets and columns corresponding to words
        """
        dics = []
        for words in preproc_X:
          dic = {}
          for word in words:
            if word not in dic:
              dic[word] = 0
            dic[word] += 1
          dics.append(dic)
        if is_test:
          return csr_matrix(self.dv.transform(dics))
        else:
          return csr_matrix(self.dv.fit_transform(dics))


    def train(self, X_train: np.ndarray[List[str]], y_train: np.ndarray[int]):
        """
        Trains the BOW classifier on the given training data.

        Parameters
        ----------
        X_train : np.ndarray[List[str]]
            Preprocessed tweets for training
        y_train : np.ndarray[int]
            Sentiments corresponding to the tweets in X_train
        """
        features = self.featurize(X_train, False)
        self.clf.fit(features, y_train)

    def test(self, X_test: np.ndarray[List[str]]) -> np.ndarray[int]:
        """
        Classifies the given test data and returns predicted sentiments.

        Parameters
        ----------
        X_test : np.ndarray[List[str]]
            Preprocessed tweets for testing

        Returns
        -------
        y_pred : np.ndarray[int]
            Predicted sentiments for the tweets in X_test
        """
        features = self.featurize(X_test, True)
        y_predict = self.clf.predict(features)
        return y_predict

"""### Step 4: Evaluate the model

To evaluate the model, we need to simulate the "real world" setting in which we have trained on our model on the data we have, but now we are using it to assign labels to data we have never seen before. We will do this using stratified k-fold cross validation.

Cross-validation is a resampling procedure used to evaluate machine learning models on a limited data sample. The procedure has a single parameter called `k` that refers to the number of groups that a given data sample is to be split into. As such, the procedure is often called k-fold cross-validation. This procedure is called "stratified" when the data is divided so that examples from each class (in this case, classes are the three sentiments) are distributed evenly among all folds.
"""

from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import accuracy_score


def run_kfold_crossval(
    model: BOW_Classifier, X: List[List[str]], y: List[int], k: int = 5
) -> List[float]:
    """
    Executes stratified k-fold cross-validation.

    Parameters
    ----------
    model : BOW_Classifier
        A BOW model that has train and test methods
    X : List[List[str]]
        Preprocessed tweets for training and testing
    y : List[int]
        Sentiments corresponding to the tweets in X
    k : int, default=5
        The number of folds to use for cross-validation

    Returns
    -------
    List[float]
        A list of accuracy values from testing with each fold
    """
    skf = StratifiedKFold(n_splits=k)
    acc = []
    bclf = BOW_Classifier()
    for train_index, test_index in skf.split(X, y):
      X_train = [X[i] for i in train_index]
      X_test = [X[i] for i in test_index]
      y_train = [y[i] for i in train_index]
      y_test = [y[i] for i in test_index]
      bclf.train(X_train, y_train)
      predicted = bclf.test(X_test)
      accuracy = accuracy_score(y_test, predicted)
      acc.append(accuracy)
    return acc

"""### Step 5: Analyze Model Performance

This function will plot how well our model is doing :)
"""

def plot_perfs(perfs: List[List[float]], names: List[str], k: int = 5):
    """
    Plots performances of models in a bar chart.

    Parameters
    ----------
    perfs : List[List[float]]
        A list of accuracy results for each model
    names : List[str]
        The names of each of the models (in the same order as perfs)
    k : int, default=5
        The value of k used for cross-validation when producing the performances
    """
    means = []
    stds = []
    for i, perf in enumerate(perfs):
        mean = np.mean(perf)
        means.append(mean)
        stds.append(np.std(perf))
        print("%s:\t%.03f" % (names[i], mean))
    plt.bar(np.arange(len(means)), means, yerr=stds)
    plt.xticks(np.arange(len(names)), names)
    plt.ylabel(f"Accuracy with {k} Folds")
    plt.ylim(0, 1)
    plt.show()

"""### Run full workflow!

"""

from collections import Counter

K_FOLD = 10
raw_tweets, y = load_data()

X_preproc = preprocess(raw_tweets)
bow_model = BOW_Classifier()
basic_bow_accs = run_kfold_crossval(bow_model, X_preproc, y, k=K_FOLD)

# here, we are going generate the "most frequent class" baseline based on the
# training data
counts = Counter(y).values()
mfc_baseline = [max(counts) / sum(counts)] * K_FOLD

# plot the results!
plot_perfs([mfc_baseline, basic_bow_accs], ["MFC Baseline", "Basic BOW"], k=K_FOLD)

"""# Part 2: Improved Preprocessing

In this section, we are going to improve the preprocessing step, but otherwise keep the above workflow the same.

### Loading Data with Spacy

The cell block below will load the dataset into spacy_processed_docs, a list of Spacy Documents that will be passed to the preprocess_part2 function below.
"""

NUM_TWEETS = 5000  # INFO: Feel free to change this to load in fewer tweets for debugging but otherwise keep it at 5000

nlp = spacy.load("en_core_web_sm")

CACHE_PATH = f"{FOLDER}/parsed_tweets.pickle"

if os.path.exists(CACHE_PATH):
    print(f"Loading parsed tweets from cache at {CACHE_PATH}")
    parsed_tweets = pickle.load(open(CACHE_PATH, "rb"))
else:
    # parse all the tweets with spacy
    parsed_tweets = []
    for i, r in enumerate(raw_tweets):
        if i == NUM_TWEETS:
            break
        parsed_tweets.append(nlp(r))
        if (i + 1) % 500 == 0:
            print(f"Processed {i + 1} out of {len(raw_tweets)}", end="\r")
    print("Processing complete", " " * 10)
    if CACHE_PATH is not None:
        pickle.dump(parsed_tweets, open(CACHE_PATH, "wb"))

print(
    f"{len(parsed_tweets)} parsed tweets loaded."
)  # This should be 5000 when not debugging

"""Please program `preprocessing_part2` to do the following things (not necessarily in this order):

- lowercasing
- lemmatization
- remove stop words
- remove punctuation and extra white space
- use only top 1000 most frequent words, and replace the rest with "\<OOV\>"
- replace numbers with "\<NUM\>"


"""

spacy_doc: TypeAlias = spacy.tokens.doc.Doc


def preprocess_part2(parsed_tweets: List[spacy_doc]) -> List[List[str]]:
    """
    Preprocesses the spacy-parsed tweets.

    Parameters
    ----------
    parsed_tweets : List[spacy_doc]
        A list of tweets parsed by spacy

    Returns
    -------
        A list of preprocessed tweets formatted as lists of tokens (lists of strings)
    """
    processed_tweets = []
    counter = Counter()

    for tweet in parsed_tweets:
      doc = nlp(tweet)
      # Lower the case, lemmatization filter out punctuation and extra whitespace tokens, remove stop words and replace numbers
      cleaned_docs = [token.lemma_.lower() if not token.like_num and not token.is_punct and not token.is_space and not token.is_stop
                      else '<NUM>' if token.like_num else None for token in doc]
      # get the frequency of each word
      counter += Counter(cleaned_docs)
      processed_tweets.append(cleaned_docs)

    most_common_words = { word for word, _ in counter.most_common(1000) }
    for tweet in processed_tweets:
      # Replace words not in the most common set with the placeholder
      tweet = [word if word in most_common_words else '<OOV>' for word in tweet]

    return processed_tweets

"""Now let's re-run the workflow and observe the difference in performance!

"""

X_preproc = preprocess_part2(parsed_tweets)
bow_model = BOW_Classifier()
better_preproc_accs = run_kfold_crossval(bow_model, X_preproc, y, k=K_FOLD)

plot_perfs(
    [mfc_baseline, basic_bow_accs, better_preproc_accs],
    ["MFC Baseline", "Basic BOW", "BOW+preproc"],
    k=K_FOLD,
)

"""# Part 3: Improved Featurization

In this section, we will expand the featurize function to include more advanced and sophisticated features. We will now add n-grams with n values from 1 to 5 as features to (partially) preserve the order of the sentence.
"""

class Better_BOW(BOW_Classifier):
    """
    A subclass of BOW_Classifier with a more complex featurization function.

    All attribute and method names match those of BOW_Classifier.
    """

    def getNgrams(self, tweet: List[str], n: int):
        """
        Returns all n-grams in a list of preprocessed tweets.

        Parameters
        ----------
        preproc_X : List[List[str]]
            A list of preprocessed tweets
        """
        dic = {}
        for i in range(len(tweet)-n+1):
            ngram = ""
            for j in range(i, i+n):
              ngram += tweet[j]
            if ngram not in dic:
              dic[ngram] = 0
            dic[ngram] += 1
        return dic



    def featurize(
        self, preproc_X: np.ndarray[List[str]], is_test: bool = False
    ) -> csr_matrix:
        """
        Turns a list of preprocessed tweets into a bag of words
        matrix using n-grams up to 5-grams.

        Parameters
        ----------
        preproc_X : List[List[str]]
            A list of preprocessed tweets
        is_test: bool, default=False
            Whether featurization should be done using features learned during training (is_test=True)
            or whether it should be done with features extracted from scratch using preproc_X (is_test=False)

        Returns
        -------
        csr_matrix
            A matrix with rows corresponding to tweets and columns corresponding to n-grams
        """
        dics = []
        for tweet in preproc_X:
          dic = self.getNgrams(tweet, 1)
          for i in range(2, 6):
            dic.update(self.getNgrams(tweet, i))
          dics.append(dic)
        if is_test:
          return csr_matrix(self.dv.transform(dics))
        else:
          return csr_matrix(self.dv.fit_transform(dics))

X_preproc = preprocess_part2(parsed_tweets)
better_bow = Better_BOW()
better_feature_accs = run_kfold_crossval(better_bow, X_preproc, y, k=K_FOLD)

plot_perfs(
    [mfc_baseline, basic_bow_accs, better_preproc_accs, better_feature_accs],
    ["MFC Baseline", "Basic BOW", "BOW+preproc", "Better Features"],
    k=K_FOLD,
)